{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, dgl, accelerate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from accelerate.utils import set_seed\n",
    "from dgl.distributed import partition_graph\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## creating top 20 pseudo csv\n",
    "top500kg = pd.read_csv('psuedo_scores_500.csv', low_memory=False)\n",
    "top500kg\n",
    "top100kg = top500kg.groupby(['y_idx', 'relation'], as_index=False, sort=False).apply(lambda x: x.head(20)).reset_index(drop=True)\n",
    "top100kg\n",
    "top100kg.to_csv('psuedo_scores_top20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top20kg[top20kg.relation == 'contraindication']\n",
    "# torch.sigmoid(torch.tensor(top100kg['score']))\n",
    "# len(top100kg[(top100kg.y_id == '1200_1134_15512_5080_100078') & (top100kg.relation == 'indication')])\n",
    "pred_x_id = top100kg[(top100kg.y_id == '1200_1134_15512_5080_100078') & (top100kg.relation == 'indication')].x_id\n",
    "top100kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('data/complex_disease_1/train.csv', low_memory=False)\n",
    "# df_train[df_train.y_id == '8099_12497_12498']\n",
    "# df_train[df_train.y_id == '8099_12497_12498']\n",
    "# df_train[df_train.y_id == '13924_12592_14672_13460_12591_12536_30861_8146_8148_32846_13459_44329_14544_9805_49223_9804_14086_8147_13515_14029_12581_19019']\n",
    "# df_train.x_type = \n",
    "# some_disease = df_train[df_train.y_id == '13924_12592_14672_13460_12591_12536_30861_8146_8148_32846_13459_44329_14544_9805_49223_9804_14086_8147_13515_14029_12581_19019']\n",
    "# some_disease\n",
    "\n",
    "dd_df_train = df_train[(df_train.y_type == 'disease') & (df_train.x_type == 'drug')]\n",
    "target_x_id = dd_df_train[(dd_df_train.y_id == '1200_1134_15512_5080_100078') & (dd_df_train.relation == 'indication')].x_id\n",
    "target_x_id\n",
    "\n",
    "print(len(set(target_x_id.values)))\n",
    "(len(set(pred_x_id.values).intersection(set(target_x_id.values))))\n",
    "# some_disease[some_disease.x_type == 'drug']\n",
    "# some_disease.x_type.unique()\n",
    "# len(df_train[df_train.y_type == 'disease'].y_id.unique())\n",
    "# df_train[df_train.y_id == '13924_12592_14672_13460_12591_12536_30861_8146']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_high_degree_disease_id_w_df(df, deg):\n",
    "    '''\n",
    "        returns the disease idx that have less than k degrees (drug-disease relation)\n",
    "    '''\n",
    "    ## extract all disease's ids\n",
    "    kg = df\n",
    "    diseases1= kg[kg['x_type'] == 'disease']['x_id']\n",
    "    diseases2 = kg[kg['y_type'] == 'disease']['y_id']\n",
    "    disease_ids = pd.concat([diseases1, diseases2]).unique()\n",
    "    len(disease_ids)\n",
    "\n",
    "    ## obtain all diseases' degree from disease-drug relation only\n",
    "    disease_drug1 = kg[(kg['x_type'] == 'disease') & (kg['y_type'] == 'drug')]['x_id']\n",
    "    disease_drug2 = kg[(kg['x_type'] == 'drug') & (kg['y_type'] == 'disease')]['y_id']\n",
    "    drugs1 = kg[(kg['x_type'] == 'disease') & (kg['y_type'] == 'drug')]['y_id']\n",
    "    drugs2 = kg[(kg['x_type'] == 'drug') & (kg['y_type'] == 'disease')]['x_id']\n",
    "    drugs_value_counts = pd.concat([drugs1, drugs2]).value_counts()\n",
    "    disease_drug_value_counts = pd.concat([disease_drug1, disease_drug2]).value_counts()\n",
    "    disease_drug_degree = disease_drug_value_counts.reindex(disease_ids).fillna(0).astype(int)\n",
    "    disease_drug_degree.sum()\n",
    "\n",
    "    ## length of ID\n",
    "    # drug_ids_x = kg[kg['x_type'] == 'drug']['x_id']\n",
    "    # drug_ids_y = kg[kg['y_type'] == 'drug']['y_id']\n",
    "    # drug_ids_value_count = pd.concat([drug_ids_x, drug_ids_y]).value_counts()\n",
    "    # drug_ids_value_count\n",
    "\n",
    "    # disease_drug_degree.index.values\n",
    "    high_disease = disease_drug_degree[disease_drug_degree > deg]\n",
    "    high_drug = drugs_value_counts[drugs_value_counts > deg]\n",
    "    return high_disease, high_drug\n",
    "\n",
    "high_disease_series, high_drug_series = obtain_high_degree_disease_id_w_df(df_train, deg=1)\n",
    "high_disease_set = set(high_disease_series.index)\n",
    "high_drug_set = set(high_drug_series.index)\n",
    "df_train[(df_train.y_id.isin(high_disease_set)) & (df_train.x_id.isin(high_drug_set))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_disease_set, high_drug_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Let's take a look at how good our pseudo labels by itself would do on the train or valid or test dataset'''\n",
    "# top500kg[top500kg.x_id.isin(set(df_train.x_id)) and top500kg.y_id.isin(set(df_train.y_id))]\n",
    "# top500kg[top500kg[['x_id', 'y_id']].isin(set(df_train[['x_id', 'y_id']].apply(tuple, axis=1)))]\n",
    "# top500kg\n",
    "# xy_pair = set(df_train[['x_id', 'y_id']].apply(tuple, axis=1))\n",
    "top500kg['x_id']\n",
    "# top500kg_in_train = top500kg.apply(lambda row: (row['x_id'], row['y_id'])).isin(xy_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_etypes = {'contraindication', 'indication', 'off-label use', 'rev_contraindication', 'rev_indication', 'rev_off-label use', }\n",
    "# df_test[df_test.relation.isin(dd_etypes)]\n",
    "df_valid[df_valid.relation.isin(dd_etypes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top500kg['score'].min() ## why no negative? because they are sorted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(torch.tensor([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = top500kg[:5]\n",
    "# test['xy_pair'] = test['x_id'] + '-' + test['y_id']\n",
    "# test['xy_pair'].isin(xy_pair)\n",
    "# Perform an inner merge on 'x_id' and 'y_id' to filter rows in 'top500kg' that exist in 'df_train'\n",
    "# df_valid =  pd.read_csv('data/complex_disease_1/valid.csv', low_memory=False)\n",
    "# df_test =  pd.read_csv('data/complex_disease_1/test.csv', low_memory=False)\n",
    "# filtered_top500kg = top500kg.merge(df_train[['x_id', 'y_id']], on=['x_id', 'y_id'], how='inner')\n",
    "filtered_valid_top500kg = top500kg.merge(df_valid[['x_id', 'y_id']], on=['x_id', 'y_id'], how='inner')\n",
    "filtered_test_top500kg = top500kg.merge(df_test[['x_id', 'y_id']], on=['x_id', 'y_id'], how='inner')\n",
    "filtered_test_top500kg\n",
    "filtered_valid_top500kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_top500kg[filtered_test_top500kg.relation == 'contraindication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean((torch.sigmoid(torch.tensor(filtered_top500kg['score'])) > 0.5).float())\n",
    "# print(torch.mean((torch.sigmoid(torch.tensor(filtered_valid_top500kg['score'])) > 0.5).float()))\n",
    "# print(torch.mean((torch.sigmoid(torch.tensor(filtered_test_top500kg['score'])) > 0.5).float()))\n",
    "print(torch.mean((torch.sigmoid(torch.tensor(filtered_test_top500kg[filtered_test_top500kg.relation == 'contraindication'].score.values)) > 0.5).float()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = pd.read_csv('data/kg.csv')\n",
    "# kg = pd.read_csv('../../PrimeKG/datasets/data/kg/auxillary/data/kg_giant.csv')\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TxData\n",
    "strt = time.time()\n",
    "TxData1 = TxData(data_folder_path = './data/')\n",
    "TxData1.prepare_split(split=split, seed=seed, no_kg=False)\n",
    "low_disease_idx = obtain_disease_idx(TxData1=TxData1, deg=deg)\n",
    "\n",
    "txGNN = TxGNN(\n",
    "            data = TxData1, \n",
    "            weight_bias_track = False,\n",
    "            proj_name = 'TxGNN',\n",
    "            exp_name = 'TxGNN'\n",
    "        )\n",
    "    \n",
    "txGNN.model_initialize(n_hid = size, \n",
    "                        n_inp = size, \n",
    "                        n_out = size, \n",
    "                        proto = True,\n",
    "                        proto_num = 3,\n",
    "                        attention = False,\n",
    "                        sim_measure = 'all_nodes_profile',\n",
    "                        bert_measure = 'disease_name',\n",
    "                        agg_measure = 'rarity',\n",
    "                        num_walks = 200,\n",
    "                        walk_mode = 'bit',\n",
    "                        path_length = 2)\n",
    "# Train\n",
    "txGNN.pretrain(n_epoch = 1, #---\n",
    "                learning_rate = 1e-3,\n",
    "                batch_size = 1024, \n",
    "                train_print_per_n = 20)\n",
    "# txGNN.finetune(n_epoch = 40, #---\n",
    "txGNN.finetune(n_epoch = 500, #---\n",
    "                learning_rate = 5e-4,\n",
    "                train_print_per_n = 5,\n",
    "                valid_per_n = 20,)\n",
    "\n",
    "disease_idxs = low_disease_idx\n",
    "txEval = TxEval(model = txGNN)\n",
    "indication = txEval.eval_disease_centric(disease_idxs = disease_idxs,\n",
    "                                        relation = 'indication',\n",
    "                                        save_name = None, \n",
    "                                        return_raw=\"concise\",\n",
    "                                        save_result = False)\n",
    "\n",
    "contraindication = txEval.eval_disease_centric(disease_idxs = disease_idxs, \n",
    "                                    relation = 'contraindication',\n",
    "                                    save_name = None, \n",
    "                                    return_raw=\"concise\",\n",
    "                                    save_result = False)\n",
    "results =  {\"indication\":indication, \"contraindication\":contraindication}\n",
    "# psuedo_training_df = turn_into_dataframe(results, t=k_top_candidates, least_score=least_score)\n",
    "psuedo_end = time.time() \n",
    "print(f\"time it took to generate psuedo_labels: {psuedo_end - strt}\")\n",
    "# return psuedo_training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txgnn import TxData, TxGNN, TxEval\n",
    "\n",
    "saving_path = './saved_models/'\n",
    "# split = 'random'\n",
    "split = 'complex_disease'\n",
    "# split = 'cell_proliferation'\n",
    "# split = 'mental_health'\n",
    "# split = 'cardiovascular'\n",
    "# split = 'anemia'\n",
    "# split = 'adrenal_gland'\n",
    "print(split)\n",
    "\n",
    "## self-supverised learning\n",
    "# additional_train = [{'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'56992.0',\t'x_idx':27422.0,\t'y_idx':19536.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'56992.0',\t'x_idx':27609.0,\t'y_idx':19536.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'56342.0',\t'x_idx':27609.0,\t'y_idx':19536.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'24234.0',\t'x_idx':24609.0,\t'y_idx':22222.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'324343.0',\t'x_idx':11111.0,\t'y_idx':19536.0,}]\n",
    "# create_psuedo_edges = True\n",
    "\n",
    "# # very_strt = time.time()\n",
    "# TxData1 = TxData(data_folder_path = './data/')\n",
    "# TxData1.prepare_split(split = split, seed = 1, no_kg = False, additional_train=additional_train, create_psuedo_edges=create_psuedo_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very_strt = time.time()\n",
    "TxData1 = TxData(data_folder_path = './data/')\n",
    "TxData1.prepare_split(split = split, seed = 1, no_kg = False, additional_train = pd.read_csv('psuedo_labels_15000.csv'))\n",
    "\n",
    "TxGNN1 = TxGNN(\n",
    "        data = TxData1, \n",
    "        weight_bias_track = False,\n",
    "        proj_name = 'TxGNN',\n",
    "        exp_name = 'TxGNN',\n",
    "    )\n",
    "\n",
    "# TxGNN1.model_initialize(n_hid = 100, \n",
    "#                       n_inp = 100, \n",
    "#                       n_out = 100, \n",
    "#                       proto = True,\n",
    "#                       proto_num = 3,\n",
    "#                       attention = False,\n",
    "#                       sim_measure = 'all_nodes_profile',\n",
    "#                       bert_measure = 'disease_name',\n",
    "#                       agg_measure = 'rarity',\n",
    "#                       num_walks = 200,\n",
    "#                       walk_mode = 'bit',\n",
    "#                       path_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract all disease's ids\n",
    "diseases1= kg[kg['x_type'] == 'disease']['x_id']\n",
    "diseases2 = kg[kg['y_type'] == 'disease']['y_id']\n",
    "disease_ids = pd.concat([diseases1, diseases2]).unique()\n",
    "len(disease_ids)\n",
    "\n",
    "## obtain all diseases' degree from disease-drug relation only\n",
    "disease_drug1 = kg[(kg['x_type'] == 'disease') & (kg['y_type'] == 'drug')]['x_id']\n",
    "disease_drug2 = kg[(kg['x_type'] == 'drug') & (kg['y_type'] == 'disease')]['y_id']\n",
    "disease_drug_value_counts = pd.concat([disease_drug1, disease_drug2]).value_counts()\n",
    "disease_drug_degree = disease_drug_value_counts.reindex(disease_ids).fillna(0).astype(int)\n",
    "disease_drug_degree.sum()\n",
    "\n",
    "## how many drugs are there?\n",
    "# drug_ids_x = kg[kg['x_type'] == 'drug']['x_id']\n",
    "# drug_ids_y = kg[kg['y_type'] == 'drug']['y_id']\n",
    "# drug_ids_value_count = pd.concat([drug_ids_x, drug_ids_y]).value_counts()\n",
    "# drug_ids_value_count\n",
    "\n",
    "disease_drug_degree.index.values\n",
    "low_disease = disease_drug_degree[disease_drug_degree < 1]\n",
    "\n",
    "id_mapping = TxData1.retrieve_id_mapping()\n",
    "id2idx = {id:idx for idx, id in id_mapping['idx2id_disease'].items()}\n",
    "print(len(id2idx))\n",
    "print(len(low_disease))\n",
    "low_disease_idx = low_disease.index.map(lambda x: id2idx[x] if '_' in x else id2idx[x+'.0'])#.apply(lambda x: id2idx[x])\n",
    "low_disease_idx = np.array(low_disease_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TxGNN1.model_initialize(n_hid = 100, \n",
    "                      n_inp = 100, \n",
    "                      n_out = 100, \n",
    "                      proto = True,\n",
    "                      proto_num = 3,\n",
    "                      attention = False,\n",
    "                      sim_measure = 'all_nodes_profile',\n",
    "                      bert_measure = 'disease_name',\n",
    "                      agg_measure = 'rarity',\n",
    "                      num_walks = 200,\n",
    "                      walk_mode = 'bit',\n",
    "                      path_length = 2)\n",
    "TxGNN1.load_pretrained('pre_trained_model_ckpt/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [{'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'56992.0',\t'x_idx':27422.0,\t'y_idx':19536.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'56992.0',\t'x_idx':34345.0,\t'y_idx':19536.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'56342.0',\t'x_idx':27422.0,\t'y_idx':19536.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'24234.0',\t'x_idx':33333.0,\t'y_idx':22222.0,},\n",
    "#          {'x_type':'gene/protein', 'x_id': '9796.0',\t'relation':'protein_protein',\t'y_type':'gene/protein',\t'y_id':'324343.0',\t'x_idx':11111.0,\t'y_idx':19536.0,}]\n",
    "# # TxData1.df_train.append(data1, ignore_index=True).drop_duplicates()\n",
    "# TxData1.df_train[TxData1.df_train.append(data1, ignore_index=True).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txgnn import TxEval\n",
    "\n",
    "# disease_idxs = [7700.0, 8000.0]\n",
    "# disease_idxs = [11764.0, 13924]\n",
    "disease_idxs = low_disease_idx[:2]\n",
    "TxEval = TxEval(model = TxGNN1)\n",
    "result = TxEval.eval_disease_centric(disease_idxs = disease_idxs, \n",
    "                                     relation = 'indication',\n",
    "                                     save_name = None, \n",
    "                                     return_raw=\"concise\",\n",
    "                                     save_result = False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unit test\n",
    "idx = np.where(np.array(result['ranked_drug_ids']['8099_12497_12498']) == 'DB00609')[0].item()\n",
    "result['ranked_scores']['8099_12497_12498'][idx]\n",
    "## 2.079557 is what we should get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test environment for get_disease_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_train, df_valid, df_test, = TxData1.df, TxData1.df_train, TxData1.df_valid, TxData1.df_test, \n",
    "disease_rel_types = ['rev_contraindication', 'rev_indication', 'rev_off-label use']\n",
    "device = torch.device(\"cuda\")\n",
    "model = TxGNN1.best_model.to(device)\n",
    "G = TxGNN1.G.to(device)\n",
    "\n",
    "def convert2str(x):\n",
    "    try:\n",
    "        if '_' in str(x): \n",
    "            pass\n",
    "        else:\n",
    "            x = float(x)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return str(x)\n",
    "\n",
    "df['x_id'] = df.x_id.apply(lambda x: convert2str(x))\n",
    "df['y_id'] = df.y_id.apply(lambda x: convert2str(x))\n",
    "\n",
    "idx2id_drug = dict(df[df.x_type == 'drug'][['x_idx', 'x_id']].drop_duplicates().values)\n",
    "idx2id_drug.update(dict(df[df.y_type == 'drug'][['y_idx', 'y_id']].drop_duplicates().values))\n",
    "\n",
    "idx2id_disease = dict(df[df.x_type == 'disease'][['x_idx', 'x_id']].drop_duplicates().values)\n",
    "idx2id_disease.update(dict(df[df.y_type == 'disease'][['y_idx', 'y_id']].drop_duplicates().values))\n",
    "\n",
    "data_path = './data/'\n",
    "df_ = pd.read_csv(os.path.join(data_path, 'kg.csv'))\n",
    "df_['x_id'] = df_.x_id.apply(lambda x: convert2str(x))\n",
    "df_['y_id'] = df_.y_id.apply(lambda x: convert2str(x))\n",
    "\n",
    "id2name_disease = dict(df_[df_.x_type == 'disease'][['x_id', 'x_name']].drop_duplicates().values)\n",
    "id2name_disease.update(dict(df_[df_.y_type == 'disease'][['y_id', 'y_name']].drop_duplicates().values))\n",
    "\n",
    "id2name_drug = dict(df_[df_.x_type == 'drug'][['x_id', 'x_name']].drop_duplicates().values)\n",
    "id2name_drug.update(dict(df_[df_.y_type == 'drug'][['y_id', 'y_name']].drop_duplicates().values))\n",
    "\n",
    "drug_ids_rels = {}\n",
    "disease_ids_rels = {}\n",
    "\n",
    "for i in ['indication', 'contraindication', 'off-label use']:\n",
    "    drug_ids_rels['rev_' + i] = df[df.relation == i].x_id.unique()\n",
    "    disease_ids_rels[i] = df[df.relation == i].y_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_disease(rel, disease_ids):\n",
    "    df_train_valid = pd.concat([df_train, df_valid])\n",
    "    df_dd = df_test[df_test.relation.isin(disease_rel_types)] ## filters df_test that has the relation of interest\n",
    "    # df_dd_all = pd.concat([df_train, df_valid, df_test]) ## \n",
    "    df_dd_train = df_train_valid[df_train_valid.relation.isin(disease_rel_types)]\n",
    "\n",
    "    df_rel_dd = df_dd[df_dd.relation == rel][['x_idx', 'y_idx']] ## shrink down df\n",
    "    df_rel_dd_train = df_dd_train[df_dd_train.relation == rel][['x_idx', 'y_idx']] ## shrink down df\n",
    "    drug_nodes = G.nodes('drug').cpu().numpy()\n",
    "    if disease_ids is None:\n",
    "        disease_ids = df_rel_dd.x_idx.unique()\n",
    "    preds_contra = {}\n",
    "    labels_contra = {}\n",
    "    ids_contra = {}\n",
    "\n",
    "    for disease_id in tqdm(disease_ids):\n",
    "\n",
    "        candidate_pos = df_rel_dd[df_rel_dd.x_idx == disease_id]#[['x_idx', 'y_idx']]\n",
    "        candidate_pos_train = df_rel_dd_train[df_rel_dd_train.x_idx == disease_id]\n",
    "        drug_pos = candidate_pos.y_idx.values\n",
    "        drug_pos_train_val = candidate_pos_train.y_idx.values\n",
    "\n",
    "        labels = {}\n",
    "        for i in drug_nodes:\n",
    "            if i in drug_pos:\n",
    "                labels[i] = 1\n",
    "            elif i in drug_pos_train_val:\n",
    "                labels[i] = -1\n",
    "                # in the training set\n",
    "            else:\n",
    "                labels[i] = 0\n",
    "\n",
    "        # construct eval graph\n",
    "        out = {}\n",
    "        src = torch.Tensor([disease_id] * len(labels)).to(device).to(dtype = torch.int64)\n",
    "        dst = torch.Tensor(list(labels.keys())).to(device).to(dtype = torch.int64)\n",
    "        out.update({('disease', rel, 'drug'): (src, dst)})\n",
    "        print(out)\n",
    "        g_eval = dgl.heterograph(out, num_nodes_dict={ntype: G.number_of_nodes(ntype) for ntype in G.ntypes}).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, pred_score_rel, _, _, _ = model(G, g_eval, psuedo=True)\n",
    "        pred = pred_score_rel[('disease', rel, 'drug')].reshape(-1,).detach().cpu().numpy()\n",
    "        lab = {idx2id_drug[i]: labels[i] for i in g_eval.edges()[1].detach().cpu().numpy()}\n",
    "        preds_contra[idx2id_disease[disease_id]] = {idx2id_drug[i]: pred[idx] for idx, i in enumerate(g_eval.edges()[1].detach().cpu().numpy())}\n",
    "        labels_contra[idx2id_disease[disease_id]] = lab\n",
    "        # ids_contra[idx2id_disease[disease_id]] = g_eval.edges()[1].detach().cpu().numpy()\n",
    "\n",
    "        del pred_score_rel\n",
    "    return preds_contra, labels_contra, drug_nodes, [id2name_drug[idx2id_drug[i]] for i in drug_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_scores(rel_type, disease_ids):\n",
    "    temp_d, preds_all, labels_all, org_out_all, metrics_all, ranked_Ids, ranked_list, name, dis_id, dis_idx, ranked_Idxs, ranked_scores = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    ## Q. rel_type = 'rev_' + relation ## why add rev_ ??\n",
    "    preds_, labels_, drug_idxs, drug_names = get_scores_disease(rel_type, disease_ids)\n",
    "    preds_all[rel_type] = preds_ ## Q. shouldn't the rel_type be in the dictionary as key then? A. No, because we are returning preds_all[relation] not preds_all\n",
    "    labels_all[rel_type] = labels_\n",
    "    ids_all = list(preds_all[rel_type].keys())\n",
    "    id2idx_disease = {id:idx for idx, id in idx2id_disease.items()}\n",
    "    for entity_id in ids_all:\n",
    "        pred = preds_all[rel_type][entity_id]\n",
    "        lab = labels_all[rel_type][entity_id]\n",
    "        ids_rels = drug_ids_rels\n",
    "        pred_array = np.array([v for k, v in pred.items()]) ## turns the dictionary into np.array, only extracting the scores\n",
    "        id2idx = {i: idx for i, idx in enumerate(pred.keys())} ## takes the index of preds_ (int) and returns the drug id (str)\n",
    "        ranked_list_entity = np.argsort(pred_array)[::-1] ## sort it in descending order\n",
    "        ranked_scores[entity_id] = pred_array[ranked_list_entity]\n",
    "        ranked_Idxs[entity_id] = ranked_list_entity\n",
    "\n",
    "        ranked_Ids[entity_id] = [id2idx[i] for i in ranked_list_entity]\n",
    "        # name[entity_id] = id2name_disease[entity_id]\n",
    "        dis_idx[entity_id] = id2idx_disease[entity_id]\n",
    "\n",
    "    out = {\"dis_idx\":dis_idx, \"ranked_drug_ids\": ranked_Ids, \"ranked_drug_idxs\": ranked_Idxs, \"ranked_scores\": ranked_scores}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_type = \"rev_\" + \"indication\"\n",
    "results = testing_scores(rel_type, disease_idxs)\n",
    "# preds_, labels_, drug_idxs, drug_names = get_scores_disease(rel_type, disease_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unit test\n",
    "len(results['ranked_scores']['8099_12497_12498']) ## should see 7957\n",
    "\n",
    "\n",
    "idx = np.where(np.array(results['ranked_drug_ids']['8099_12497_12498']) == 'DB00609')[0].item()\n",
    "results['ranked_scores']['8099_12497_12498'][idx]\n",
    "## 2.079557 is what we should get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, dgl, accelerate\n",
    "from txgnn import TxData, TxGNN, TxEval\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from accelerate.utils import set_seed\n",
    "# from dgl.distributed import partition_graph\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "# saving_path = './saved_models/'\n",
    "# split = 'random'\n",
    "# split = 'complex_disease'\n",
    "# split = 'cell_proliferation'\n",
    "# split = 'mental_health'\n",
    "# split = 'cardiovascular'\n",
    "# split = 'anemia'\n",
    "# split = 'adrenal_gland'\n",
    "# print(split)\n",
    "\n",
    "'''\n",
    "    Let's first try one iteration to increase performance.\n",
    "'''\n",
    "\n",
    "def obtain_disease_idx(TxData1, deg):\n",
    "    '''\n",
    "        returns the disease idx that have less than k degrees (drug-disease relation)\n",
    "    '''\n",
    "    ## extract all disease's ids\n",
    "    kg = pd.read_csv('data/kg.csv')\n",
    "    diseases1= kg[kg['x_type'] == 'disease']['x_id']\n",
    "    diseases2 = kg[kg['y_type'] == 'disease']['y_id']\n",
    "    disease_ids = pd.concat([diseases1, diseases2]).unique()\n",
    "    len(disease_ids)\n",
    "\n",
    "    ## obtain all diseases' degree from disease-drug relation only\n",
    "    disease_drug1 = kg[(kg['x_type'] == 'disease') & (kg['y_type'] == 'drug')]['x_id']\n",
    "    disease_drug2 = kg[(kg['x_type'] == 'drug') & (kg['y_type'] == 'disease')]['y_id']\n",
    "    disease_drug_value_counts = pd.concat([disease_drug1, disease_drug2]).value_counts()\n",
    "    disease_drug_degree = disease_drug_value_counts.reindex(disease_ids).fillna(0).astype(int)\n",
    "    disease_drug_degree.sum()\n",
    "\n",
    "    ## length of ID\n",
    "    # drug_ids_x = kg[kg['x_type'] == 'drug']['x_id']\n",
    "    # drug_ids_y = kg[kg['y_type'] == 'drug']['y_id']\n",
    "    # drug_ids_value_count = pd.concat([drug_ids_x, drug_ids_y]).value_counts()\n",
    "    # drug_ids_value_count\n",
    "\n",
    "    disease_drug_degree.index.values\n",
    "    low_disease = disease_drug_degree[disease_drug_degree < deg]\n",
    "\n",
    "    id_mapping = TxData1.retrieve_id_mapping()\n",
    "    id2idx = {id:idx for idx, id in id_mapping['idx2id_disease'].items()}\n",
    "    print(f\"Total number of diseases?: {len(id2idx)}\")\n",
    "    print(f\"total number of {deg}> degree diseases?: {len(low_disease)}\")\n",
    "    low_disease_idx = low_disease.index.map(lambda x: id2idx[x] if '_' in x else id2idx[x+'.0'])#.apply(lambda x: id2idx[x])\n",
    "    low_disease_idx = np.array(low_disease_idx)\n",
    "\n",
    "    return low_disease_idx[:2] ## testing\n",
    "\n",
    "def turn_into_dataframe(results, t, least_score):\n",
    "    '''\n",
    "        t = number of psuedo_labels to be generated for low_diseases\n",
    "        Takes in the results eval file and returns the disease idx that have less than k degrees (drug-disease relation)\n",
    "    '''\n",
    "    additional_train_dict = []\n",
    "    for rel, result in results.items():\n",
    "        for (dis_id, drug_ids), drug_idxs, dis_idx, ranked_scores in zip(result['ranked_drug_ids'].items(), result['ranked_drug_idxs'].values(), result['dis_idx'].values(), result['ranked_scores'].values()):\n",
    "            if least_score is None:\n",
    "                new_dicts = [{'y_id': dis_id, 'y_idx': dis_idx, 'x_id': drug_id, 'x_idx': drug_idx, 'relation': rel} for i, (drug_id, drug_idx) in enumerate(zip(drug_ids, drug_idxs)) if i < t]\n",
    "            else:\n",
    "                new_dicts = [{'y_id': dis_id, 'y_idx': dis_idx, 'x_id': drug_id, 'x_idx': drug_idx, 'relation': rel} for i, (drug_id, drug_idx, ranked_score) in enumerate(zip(drug_ids, drug_idxs, ranked_scores)) if ranked_score > least_score]\n",
    "            additional_train_dict += new_dicts\n",
    "\n",
    "    df = pd.DataFrame(additional_train_dict)\n",
    "    df[\"x_idx\"] = df[\"x_idx\"].astype(float)\n",
    "    df[\"y_type\"] = \"disease\"\n",
    "    df[\"x_type\"] = \"drug\"\n",
    "    return df\n",
    "\n",
    "def generate_psuedo_labels(pre_trained_dir='pre_trained_model_ckpt/1', split='complex_disease', size=100, seed=1, deg=1, k_top_candidates=5, least_score=None):\n",
    "    '''\n",
    "        Loads a pre-trained model, calls (obtain_disease_idx, turn_into_dataframe) to generates psuedo_labels for diseases less than 'deg'. Returns dataframe ready to be augmented to df_train.\n",
    "    '''\n",
    "    strt = time.time()\n",
    "    TxData1 = TxData(data_folder_path = './data/')\n",
    "    TxData1.prepare_split(split=split, seed=seed, no_kg=False)\n",
    "    low_disease_idx = obtain_disease_idx(TxData1=TxData1, deg=deg)\n",
    "\n",
    "    txGNN = TxGNN(\n",
    "                data = TxData1, \n",
    "                weight_bias_track = False,\n",
    "                proj_name = 'TxGNN',\n",
    "                exp_name = 'TxGNN'\n",
    "            )\n",
    "        \n",
    "    txGNN.model_initialize(n_hid = size, \n",
    "                            n_inp = size, \n",
    "                            n_out = size, \n",
    "                            proto = True,\n",
    "                            proto_num = 3,\n",
    "                            attention = False,\n",
    "                            sim_measure = 'all_nodes_profile',\n",
    "                            bert_measure = 'disease_name',\n",
    "                            agg_measure = 'rarity',\n",
    "                            num_walks = 200,\n",
    "                            walk_mode = 'bit',\n",
    "                            path_length = 2)\n",
    "    txGNN.load_pretrained(pre_trained_dir)\n",
    "    disease_idxs = low_disease_idx\n",
    "    txEval = TxEval(model = txGNN)\n",
    "    indication = txEval.eval_disease_centric(disease_idxs = disease_idxs,\n",
    "                                         relation = 'indication',\n",
    "                                         save_name = None, \n",
    "                                         return_raw=\"concise\",\n",
    "                                         save_result = False)\n",
    "    \n",
    "    contraindication = txEval.eval_disease_centric(disease_idxs = disease_idxs, \n",
    "                                        relation = 'contraindication',\n",
    "                                        save_name = None, \n",
    "                                        return_raw=\"concise\",\n",
    "                                        save_result = False)\n",
    "    results =  {\"indication\":indication, \"contraindication\":contraindication}\n",
    "    psuedo_training_df = turn_into_dataframe(results, t=k_top_candidates, least_score=least_score)\n",
    "    psuedo_end = time.time() \n",
    "    print(f\"time it took to generate psuedo_labels: {psuedo_end - strt}\")\n",
    "    return psuedo_training_df\n",
    "\n",
    "def train_w_psuedo_labels(size=100, split='complex_disease', additional_train=None, create_psuedo_edges=False, seed=1, save_dir=None, dropout=0, reparam_mode=False, weight_decay=0):\n",
    "    '''\n",
    "        Takes in pretrained model and generate psuedo label? \n",
    "    '''\n",
    "    strt = time.time()\n",
    "    TxData1 = TxData(data_folder_path = './data/')\n",
    "    ## add additional psuedo-training labels\n",
    "    TxData1.prepare_split(split=split, seed=seed, no_kg=False, additional_train=additional_train, create_psuedo_edges=create_psuedo_edges,)\n",
    "    TxGNN1 = TxGNN(\n",
    "            data = TxData1, \n",
    "            weight_bias_track = False, #True,\n",
    "            proj_name = 'TxGNN',\n",
    "            exp_name = 'TxGNN'\n",
    "        )\n",
    "    TxGNN1.model_initialize(n_hid = size, \n",
    "                            n_inp = size, \n",
    "                            n_out = size, \n",
    "                            proto = True,\n",
    "                            proto_num = 3,\n",
    "                            attention = False,\n",
    "                            sim_measure = 'all_nodes_profile',\n",
    "                            bert_measure = 'disease_name',\n",
    "                            agg_measure = 'rarity',\n",
    "                            num_walks = 200,\n",
    "                            walk_mode = 'bit',\n",
    "                            path_length = 2,\n",
    "                            dropout=dropout,\n",
    "                            reparam_mode=reparam_mode)\n",
    "    # Train\n",
    "    TxGNN1.pretrain(n_epoch = 1, #---\n",
    "                    learning_rate = 1e-3,\n",
    "                    batch_size = 1024, \n",
    "                    train_print_per_n = 20)\n",
    "    TxGNN1.finetune(n_epoch = 500, #---\n",
    "                    learning_rate = 5e-4,\n",
    "                    train_print_per_n = 5,\n",
    "                    valid_per_n = 20,\n",
    "                    weight_decay = weight_decay,)\n",
    "    print(f\"time it took for this training iteration: {time.time() - strt}\")\n",
    "    if save_dir is not None:\n",
    "        noisy_student_fpath = './Noisy_student/'\n",
    "        TxGNN1.save_model(path = noisy_student_fpath+save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dropout', default=0.0)\n",
    "parser.add_argument('--reparam_mode', default=False, help='choose from {MLP, RMLP, MPNN}')\n",
    "parser.add_argument('--psuedo_label_fname', default=None, help='choose from {psuedo_labels_75000.csv, }') ## is default None? \n",
    "parser.add_argument('--split', default='complex_disease', help='choose from {complex_disease, ...}')\n",
    "parser.add_argument('--weight_decay', default=0.0, type=float)\n",
    "parser.add_argument('--k_top_candidates', default=5, type=int)\n",
    "parser.add_argument('--psuedo_edges', action='store_true')\n",
    "parser.add_argument('--train_from_scratch', action='store_true')\n",
    "parser.add_argument('--student_size', default=120, type=int)\n",
    "parser.add_argument('--three_iter_from_scratch', action='store_true')\n",
    "parser.add_argument('--use_diff_savedir', action='store_true')\n",
    "parser.add_argument('--scaling_psuedo', action='store_true')\n",
    "parser.add_argument('--testing', action='store_true')\n",
    "parser.add_argument('--deg', default=1, type=float) ## 'inf' for all diseases?\n",
    "parser.add_argument('--more_than_one_model_per_script', action='store_true')\n",
    "parser.add_argument('--least_score', default=None, type=float)\n",
    "\n",
    "## pre_trained_model\n",
    "args, _ = parser.parse_known_args()\n",
    "args.psuedo_label_fname = 'pl_least_score_5.csv'\n",
    "args.least_score = 5.0\n",
    "\n",
    "save_dir = 'Noisy_student2' if args.use_diff_savedir else 'Noisy_student'\n",
    "\n",
    "seed = 1\n",
    "if args.three_iter_from_scratch:\n",
    "    # seed = random.randint(2, 100)\n",
    "    print(f\"Using seed{seed} to do full three iteration training\")\n",
    "    train_w_psuedo_labels(split=args.split, seed=seed, save_dir=\"Teacher\")\n",
    "    for i in range(3):\n",
    "        print(f\"generating and training Student{i+1}\")\n",
    "        args.k_top_candidates = args.k_top_candidates * (i+1) if args.scaling_psuedo else args.k_top_candidates ## scale num of psuedo labels\n",
    "        size = 100 if i == 0 else args.student_size\n",
    "        pre_trained_dir = f'./{save_dir}/Student{i}' if i > 0 else f\"./{save_dir}/Teacher\"\n",
    "        psuedo_labels = generate_psuedo_labels(pre_trained_dir=pre_trained_dir, split=args.split, size=size, seed=seed, deg=args.deg, k_top_candidates=args.k_top_candidates)\n",
    "        train_w_psuedo_labels(size=args.student_size, \n",
    "                        dropout=args.dropout, \n",
    "                        save_dir=f\"Student{i+1}/\", \n",
    "                        additional_train=psuedo_labels, \n",
    "                        create_psuedo_edges=args.psuedo_edges, \n",
    "                        split=args.split, \n",
    "                        reparam_mode=args.reparam_mode,\n",
    "                        weight_decay=args.weight_decay)\n",
    "# elif args.testing:\n",
    "#     for i in range(2):\n",
    "#         train_w_psuedo_labels(size=args.student_size, \n",
    "#                         dropout=args.dropout, \n",
    "#                         save_dir=\"The_First_Student/\", \n",
    "#                         additional_train=None, \n",
    "#                         create_psuedo_edges=args.psuedo_edges, \n",
    "#                         split=args.split, \n",
    "#                         seed=seed,\n",
    "#                         reparam_mode=args.reparam_mode,\n",
    "#                         weight_decay=args.weight_decay)\n",
    "elif args.train_from_scratch: ## To evaluate model at different seed\n",
    "        ## should not be used until fixes\n",
    "        seed = random.randint(2, 100)\n",
    "        print(f\"randomly picked seed is: {seed}\")\n",
    "        train_w_psuedo_labels(split=args.split, seed=seed, save_dir=\"random_seed\")\n",
    "        psuedo_labels = generate_psuedo_labels(pre_trained_dir=f'./{save_dir}/random_seed', split=args.split, size=100, seed=seed, deg=args.deg, k_top_candidates=args.k_top_candidates)\n",
    "elif args.psuedo_label_fname is not None and os.path.exists(args.psuedo_label_fname):\n",
    "    psuedo_labels = pd.read_csv(args.psuedo_label_fname)\n",
    "    print(f\"Loading generated psuedo_labels with size: {len(psuedo_labels)}\")\n",
    "    train_w_psuedo_labels(size=args.student_size, \n",
    "                        dropout=args.dropout, \n",
    "                        save_dir=\"The_First_Student/\", \n",
    "                        additional_train=psuedo_labels, \n",
    "                        create_psuedo_edges=args.psuedo_edges, \n",
    "                        split=args.split, \n",
    "                        seed=seed,\n",
    "                        reparam_mode=args.reparam_mode,\n",
    "                            weight_decay=args.weight_decay)\n",
    "else:\n",
    "    print(f\"Only generating {args.psuedo_label_fname}\")\n",
    "    psuedo_labels = generate_psuedo_labels(pre_trained_dir='pre_trained_model_ckpt/1', split=args.split, size=100, seed=seed, deg=args.deg, k_top_candidates=args.k_top_candidates, least_score=args.least_score)\n",
    "    psuedo_labels.to_csv(args.psuedo_label_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psuedo_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
